{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Constants import Constants\n",
    "import Utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import json\n",
    "from sklearn.preprocessing import KBinsDiscretizer, quantile_transform\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cvap', 'net_dem_president_votes', 'urm_pct'] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'against_sah_rt_quantiles': [142, 24, 0, 0, 0],\n",
       "  'cases_per_capita_quantiles': [213, 219, 194, 177, 156],\n",
       "  'cvap_quantiles': [0, 13, 39, 56, 851],\n",
       "  'deaths_per_capita_quantiles': [236, 204, 187, 149, 183],\n",
       "  'for_sah_rt_quantiles': [618, 175, 0, 0, 0],\n",
       "  'is_blue': 587,\n",
       "  'negative_sentiment': 487,\n",
       "  'net_dem_president_votes_quantiles': [249, 50, 29, 21, 610],\n",
       "  'positive_sentiment': 472,\n",
       "  'total_tweets': 959,\n",
       "  'urm_pct_quantiles': [11, 47, 189, 368, 344]},\n",
       " {'against_sah_rt_quantiles': [26, 6, 0, 0, 0],\n",
       "  'cases_per_capita_quantiles': [41, 36, 53, 60, 71],\n",
       "  'cvap_quantiles': [0, 0, 0, 0, 261],\n",
       "  'deaths_per_capita_quantiles': [31, 39, 59, 64, 68],\n",
       "  'for_sah_rt_quantiles': [177, 52, 0, 0, 0],\n",
       "  'is_blue': 249,\n",
       "  'negative_sentiment': 130,\n",
       "  'net_dem_president_votes_quantiles': [12, 0, 0, 0, 249],\n",
       "  'positive_sentiment': 131,\n",
       "  'total_tweets': 261,\n",
       "  'urm_pct_quantiles': [0, 0, 8, 33, 220]},\n",
       " {'against_sah_rt_quantiles': [19, 2, 0, 0, 0],\n",
       "  'cases_per_capita_quantiles': [23, 33, 27, 37, 37],\n",
       "  'cvap_quantiles': [0, 0, 0, 0, 157],\n",
       "  'deaths_per_capita_quantiles': [19, 37, 33, 57, 11],\n",
       "  'for_sah_rt_quantiles': [105, 31, 0, 0, 0],\n",
       "  'is_blue': 119,\n",
       "  'negative_sentiment': 80,\n",
       "  'net_dem_president_votes_quantiles': [38, 0, 0, 0, 119],\n",
       "  'positive_sentiment': 77,\n",
       "  'total_tweets': 157,\n",
       "  'urm_pct_quantiles': [0, 0, 0, 0, 157]},\n",
       " {'against_sah_rt_quantiles': [14, 1, 0, 0, 0],\n",
       "  'cases_per_capita_quantiles': [20, 8, 23, 22, 33],\n",
       "  'cvap_quantiles': [0, 0, 0, 0, 106],\n",
       "  'deaths_per_capita_quantiles': [11, 16, 18, 26, 35],\n",
       "  'for_sah_rt_quantiles': [62, 29, 0, 0, 0],\n",
       "  'is_blue': 106,\n",
       "  'negative_sentiment': 54,\n",
       "  'net_dem_president_votes_quantiles': [0, 0, 0, 0, 106],\n",
       "  'positive_sentiment': 52,\n",
       "  'total_tweets': 106,\n",
       "  'urm_pct_quantiles': [0, 0, 0, 0, 106]}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_static_county_data(file=Constants.static_county_data_output_file):\n",
    "    try:\n",
    "        with open(file,'r') as f:\n",
    "            df = json.load(f)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        df = processed_county_demographics()\n",
    "        df.loc[:,'features'] = df.geometry.apply(lambda x: mapping(x))\n",
    "        return pd.DataFrame(df).drop(['geometry'],axis=1).to_dict(orient='index')\n",
    "    \n",
    "def load_census_df(file=Constants.static_county_data_output_file):\n",
    "    df = pd.DataFrame(load_static_county_data()).T\n",
    "    df.index.name = 'GEOID'\n",
    "    df = df.drop(['county_fips','features'],axis=1).reset_index()\n",
    "    df.GEOID = df.GEOID.astype(int)\n",
    "    return df\n",
    "\n",
    "def load_county_border_dict(border_json=Constants.static_county_data_output_file):\n",
    "    #should give {geoid: <geojson item> for each county}\n",
    "    try:\n",
    "        with open(border_json,'r') as f:\n",
    "            border_dict = json.load(f)\n",
    "        borders = pd.DataFrame(border_dict).T\n",
    "        borders.index.name = \"GEOID\"\n",
    "        return borders.features.to_dict()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def load_aggregate_border_dict(border_json=Constants.aggregated_county_border_output_file):\n",
    "    #should give {geoid: <geojson item> for each group of countys}\n",
    "    try:\n",
    "        with open(border_json,'r') as f:\n",
    "            aggregate_borders = json.load(f)\n",
    "        return aggregate_borders\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "class TweetClusterer():\n",
    "    \n",
    "    def __init__(self,fields,id_field='case_id', n_clusters=4, quant_bins=30):\n",
    "        self.fields = fields\n",
    "        self.bins= quant_bins\n",
    "        self.id_field=id_field\n",
    "        self.n_clusters = n_clusters\n",
    "        self.discretizer = KBinsDiscretizer(quant_bins,encode='ordinal')\n",
    "        self.clusterer = AgglomerativeClustering(n_clusters,linkage='ward')\n",
    "        \n",
    "    def transform(self,x):\n",
    "        return self.clusterer.fit_predict(x)\n",
    "    \n",
    "    def fit_kbins(self,x):\n",
    "        self.discretizer.fit(x)\n",
    "        \n",
    "    def valid_fields(self,field_lists):\n",
    "        #returns the fields in self.fields that are in a list of lists of columns\n",
    "        #for figuring out what fields to reguarlize\n",
    "        to_keep = set(self.fields)\n",
    "        for field_list in field_lists:\n",
    "            to_keep = to_keep.intersection(set(field_list))\n",
    "        return sorted(to_keep)\n",
    "        \n",
    "    def transform_df(self,tweet_df, county_df):\n",
    "        #dataframe should have an id as an index for this to work good with linking in UI\n",
    "        #will calculate quantiles based on the county\n",
    "        try:\n",
    "            #figure out what data is in the demographics so we can regularize it - so we ignore derived stuff\n",
    "            regularize_fields = self.valid_fields([tweet_df.columns,county_df.columns])\n",
    "            default_fields = sorted(set(self.fields) - set(regularize_fields))\n",
    "            print(regularize_fields,default_fields)\n",
    "            cdata = county_df.loc[:,regularize_fields].values\n",
    "            self.fit_kbins(cdata)\n",
    "            \n",
    "            #regularize fields based on county-level quantiles\n",
    "            tdf = tweet_df.copy() \n",
    "            transform_tdata = tdf.loc[:,regularize_fields].values\n",
    "            tdf.loc[:,regularize_fields] = self.discretizer.transform(transform_tdata)\n",
    "            \n",
    "            #but we are clustering the tweets\n",
    "            tdata = tweet_df.loc[:,self.fields].values\n",
    "            tindex = tdf.reset_index().loc[:,self.id_field]\n",
    "            \n",
    "            tclusters = self.transform(tdata)\n",
    "            \n",
    "            #should be 2 columns: cluster and the class id_field (for merging later)\n",
    "            cluster_series = pd.DataFrame(tclusters,index=tindex,columns=['cluster'])\n",
    "            cluster_series = cluster_series.reset_index()\n",
    "#             self.current_clusters = cluster_series\n",
    "            return cluster_series\n",
    "        except Exception as e:\n",
    "            print('error in transform_df', e)\n",
    "            \n",
    "    def add_clusters(self,augmented_tweet_df,county_df):\n",
    "        cluster_df = self.transform_df(augmented_tweet_df, county_df)\n",
    "        index = augmented_tweet_df.index.name\n",
    "        df = augmented_tweet_df.reset_index().merge(cluster_df,on=self.id_field)\n",
    "        if index is not None:\n",
    "            df = df.set_index(index)\n",
    "        return df\n",
    "\n",
    "class DataProcessor():\n",
    "    \n",
    "    def __init__(self, quant_bins = 10):\n",
    "        self.quant_bins = quant_bins\n",
    "        self.demographic_df = load_census_df()\n",
    "        self.demographic_fields = self.get_demographic_fields(self.demographic_df)\n",
    "        \n",
    "        self.tweet_df = pd.read_csv(Constants.tweet_output_file)#,dtype={'GEOID':int})\n",
    "        covid_df = pd.read_csv(Constants.covid_cases_output_file)#,dtype={'GEOID':int})\n",
    "        self.covid_df = covid_df.drop('Unnamed: 0', axis=1)\n",
    "        \n",
    "        self.augmented_tweet_df = self.augment_tweets()\n",
    "        \n",
    "        #currently loads in as a dict {id:<geojson string>,...}\n",
    "        self.county_borders = load_county_border_dict()\n",
    "        self.aggregate_borders = load_aggregate_border_dict()\n",
    "        \n",
    "        self.frames = self.get_frames()\n",
    "        self.current_tweet_clusters = None\n",
    "        \n",
    "    def get_frames(self):\n",
    "        frames = ['Authority','Betrayal',\n",
    "                  'Care','Degradation',\n",
    "                  'Fairness','Freedom',\n",
    "                  'Harm','Injustice',\n",
    "                  'Loyalty','Oppression',\n",
    "                  'Purity','Subversion'\n",
    "                 ]\n",
    "        return frames\n",
    "    \n",
    "    def get_demographic_fields(self, demographic_df):\n",
    "        #extract columns that are census/county numerical data\n",
    "        d_fields = set(demographic_df.columns)\n",
    "        extra_fields = set(['GEOID','parent','state_fips','county_name'])\n",
    "        return list(d_fields - extra_fields)\n",
    "    \n",
    "    def augment_tweets(self):\n",
    "        ddf = self.demographic_df.copy() #bin_edges = self.quantize_demographics(self.demographic_df, self.quant_bins) \n",
    "        tdf = self.tweet_df.copy()\n",
    "        cdf = self.covid_df.copy()\n",
    "        \n",
    "        ddf_to_drop = list(set(ddf.columns).intersection(set(['county_name','state_fips'])))\n",
    "        mdf = tdf.merge(ddf.drop(ddf_to_drop,axis=1),on='GEOID',how='left')\n",
    "        mdf = mdf.merge(cdf,on=['GEOID','day','month','year'],how='left')\n",
    "        \n",
    "        mdf.loc[:,'is_blue'] = (mdf.net_dem_president_votes > 0).astype(int)\n",
    "        mdf.loc[:,'cases_per_capita'] = (mdf.cases/mdf.cvap)\n",
    "        mdf.loc[:,'deaths_per_capita'] = (mdf.deaths/mdf.cvap)\n",
    "        return mdf[mdf.GEOID != 0]\n",
    "\n",
    "    def stratify_retweet_thresholds(self, n_quantiles=10, retweet_col='retweet_count'):\n",
    "        tdf = self.tweet_df.copy()\n",
    "        retweets = tdf[tdf[retweet_col] > 1]\n",
    "        retweets = retweets[retweet_col]\n",
    "        quantile_edges =np.quantile(retweets,[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,.99],interpolation='nearest')\n",
    "        quantile_edges = sorted(set([0,1]).union(set(quantile_edges)))\n",
    "        quantile_edges = [int(q)  for q in quantile_edges]\n",
    "        return quantile_edges\n",
    "\n",
    "    def filter_geolocated(self, df):\n",
    "        return df[df.GEOID != 0].copy().fillna(-1)\n",
    "    \n",
    "    def format_frameview_df(self, month=None,year=None):\n",
    "        #file to get data formated for the moral-frame view.  need to add int sentiment later\n",
    "        #only geolocated tweets I guess\n",
    "        tweet_df = self.filter_geolocated(self.augmented_tweet_df)\n",
    "        #filter by month if needed.  I think I am not planning on doing that tho\n",
    "        if month is not None:\n",
    "            tweet_df = tweet_df[(tweet_df.month == int(month))]\n",
    "        if year is not None:\n",
    "            tweet_df = tweet_df[(tweet_df.year == int(year))]\n",
    "\n",
    "        quantile_thresholds = self.stratify_retweet_thresholds(self.tweet_df.copy())\n",
    "        frame_data = {}\n",
    "\n",
    "        for frame in self.frames:\n",
    "            frame_df = tweet_df[tweet_df[frame] > 0]\n",
    "            entry = {'total_tweets': frame_df.shape[0]}\n",
    "            for_rt_quantiles = []\n",
    "            against_rt_quantiles = []\n",
    "            for idx, q in enumerate(quantile_thresholds):\n",
    "\n",
    "                tweets_in_quantile = frame_df[frame_df.retweet_count >= q]\n",
    "                if idx + 1 < len(quantile_thresholds):\n",
    "                    maxval = quantile_thresholds[idx+1]\n",
    "                    tweets_in_quantile = tweets_in_quantile[tweets_in_quantile.retweet_count < maxval]\n",
    "\n",
    "                for_sah_q = (tweets_in_quantile.for_sah > 0).sum()\n",
    "                against_sah_q = (tweets_in_quantile.for_sah <= 0).sum()\n",
    "\n",
    "                for_rt_quantiles.append(for_sah_q)\n",
    "                against_rt_quantiles.append(against_sah_q)\n",
    "\n",
    "            entry['vivid'] = frame_df.is_vivid.sum()\n",
    "            entry['for_sah'] = frame_df.for_sah.sum()\n",
    "            entry['is_blue'] = frame_df.is_blue.sum()\n",
    "            entry['for_sah_rt_quantiles'] = for_rt_quantiles\n",
    "            entry['against_sah_rt_quantiles'] = against_rt_quantiles\n",
    "            entry['positive_sentiment'] = (frame_df.sentiment_score > .05).sum()\n",
    "            entry['negative_sentiment'] = (frame_df.sentiment_score < .05).sum()\n",
    "            frame_data[frame] = entry\n",
    "        return pd.DataFrame(frame_data).T.fillna(-1)\n",
    "    \n",
    "    def load_frameview_dict(self,month=None,year=None):\n",
    "        #loads in the data for using in the view with overview dat afor each frame\n",
    "        frame_df = self.format_frameview_df(month,year)\n",
    "        return frame_df.to_dict(orient='index')\n",
    "    \n",
    "    def load_timeline_dict(self,frame=None,min_retweets = 0, month=None,year=None):\n",
    "        #get data to use for the big timeline.  Should be similar to the default format for tweets?\n",
    "        #should be like: {day: [tweets in day as a json object], day2: [...]}\n",
    "        df = self.filter_geolocated(self.augmented_tweet_df)\n",
    "        \n",
    "        #drop extra features, but check that they're in the columns because there's no good way to do that in pandas?\n",
    "        to_drop = ['screen_name','user_id'] + self.demographic_fields\n",
    "        df = df.drop(list(set(df.columns).intersection(set(to_drop))),axis=1)\n",
    "        print(to_drop, df.columns)\n",
    "        #filter stuff to the selected month and frame\n",
    "        if frame is not None:\n",
    "            df = df[df[frame] == 1]\n",
    "        if month is not None:\n",
    "            df = df[df.month == month]\n",
    "        if year is not None:\n",
    "            df = df[df.year == year]\n",
    "        #in case we want only popular tweets\n",
    "        df = df[df.retweet_count >= min_retweets].drop(['month','year','GEOID','parent'],axis=1)\n",
    "        tweet_dict = {day:d.to_dict(orient='records') for day,d in df.groupby('day')}\n",
    "        return tweet_dict\n",
    "    \n",
    "    def aggregate_county_demographics(self,ddf):\n",
    "        #should return a dataframe with aggreagted demographics and the parent congressional district\n",
    "        #will only keep the demographic fields for now\n",
    "        d_fields = self.demographic_fields\n",
    "        ddf.loc[:,d_fields] = ddf.loc[:,d_fields].astype(float)\n",
    "        sum_fields = ['cvap','net_dem_gov_votes','net_dem_president_votes','repgov']\n",
    "        median_fields = ['ruralurban_cc']\n",
    "        wmean_fields = sorted(set(d_fields) - set(sum_fields) - set(median_fields))\n",
    "        district_data = {}\n",
    "        for district, sub_df in ddf.groupby('parent'):\n",
    "            entry = {}\n",
    "            medians = sub_df.loc[:,median_fields].median().to_dict()\n",
    "            sums = sub_df.loc[:,sum_fields].sum().to_dict()\n",
    "            wmeans = {}\n",
    "            total_pop = sub_df.cvap.sum()\n",
    "            for wmf in wmean_fields:\n",
    "                values = sub_df[wmf]*sub_df.cvap\n",
    "                entry[wmf] = values.sum()/total_pop\n",
    "            for val_dict in [medians, sums, wmeans]:\n",
    "                for k,v in val_dict.items():\n",
    "                    entry[k] = v\n",
    "            district_data[district] = entry\n",
    "        district_df = pd.DataFrame(district_data).T\n",
    "        district_df.index.name = 'parent'\n",
    "        return district_df.reset_index()\n",
    "        \n",
    "    def format_county_data(self, month=None,year=None,aggregate = False):\n",
    "        #data for the county map\n",
    "        tdf = self.filter_geolocated(self.augmented_tweet_df.copy())\n",
    "        if month is not None:\n",
    "            tdf = tdf[tdf.month == month]\n",
    "        if year is not None:\n",
    "            tdf = tdf[tdf.year == year]\n",
    "            \n",
    "        ddf = self.demographic_df.copy()\n",
    "        #aggregates county census data by the containing congressional ditrict.\n",
    "        if aggregate:\n",
    "            ddf = self.aggregate_county_demographics(ddf)\n",
    "        key = 'GEOID' if aggregate is False else 'parent'\n",
    "        ddf = ddf.set_index(key)\n",
    "#         ddf.index = ddf.index.astype(int)\n",
    "        \n",
    "        frames = self.get_frames()\n",
    "        \n",
    "        data_dict = {}\n",
    "        for geoid, df in tdf.groupby(key):\n",
    "#             geoid=int(geoid)\n",
    "            entry = df.loc[:,frames].sum().to_dict()\n",
    "            entry['total_tweets'] = df.shape[0]\n",
    "            #average sentiment weighted be popularity of the tweet\n",
    "            retweet_weights = (df.retweet_count+2).apply(np.log)\n",
    "            entry['weighted_sentiment'] = ((retweet_weights*df.sentiment_score).sum())/(retweet_weights.sum())\n",
    "        \n",
    "            entry['median_deaths'] = df.loc[:,'deaths'].median()\n",
    "            entry['median_cases'] = df.loc[:,'cases'].median()\n",
    "            \n",
    "            calc_change = lambda key: df.loc[:,key].max() - df.loc[:,key].min()\n",
    "            entry['cases_change'] = calc_change('cases')\n",
    "            entry['deaths_change'] = calc_change('deaths')\n",
    "            demographics = ddf.loc[geoid]\n",
    "            for dem in self.demographic_fields:\n",
    "                entry[dem] = demographics[dem]\n",
    "            data_dict[geoid] = entry\n",
    "        data = pd.DataFrame(data_dict)\n",
    "        return data\n",
    "    \n",
    "    def load_county_dict(self, month=None,year=None,aggregate=False):\n",
    "        #issue: I'm not aggregating stuff properly????????\n",
    "        county_df = self.format_county_data(month,year,aggregate).T\n",
    "        if aggregate:\n",
    "            map_json = self.aggregate_borders\n",
    "        else:\n",
    "            map_json = self.county_borders\n",
    "        return county_df.to_dict(orient='index')\n",
    "    \n",
    "    def tweets_in_cluster(self,tweet_id):\n",
    "        #given a tweet id (currently case_id), gives the list of ids with the same cluster\n",
    "        #for linking in the UI\n",
    "        cc = self.current_tweet_clusters\n",
    "        if cc is None:\n",
    "            return []\n",
    "        active_cluster = cc.loc[tweet_id].cluster\n",
    "        active_tweets = cc[cc.cluster == active_cluster]\n",
    "        return active_tweets.index.to_list()\n",
    "    \n",
    "    def get_tweet_clusters(self,cluster_fields,n_clusters,identifier='case_id'):\n",
    "        clusterer = TweetClusterer(cluster_fields, identifier, n_clusters)\n",
    "        \n",
    "        tdf = self.filter_geolocated(self.augmented_tweet_df)\n",
    "        tdf = tdf.drop(self.get_frames(),axis=1)\n",
    "        tdf = clusterer.add_clusters(tdf,self.demographic_df)\n",
    "        self.current_tweet_clusters = tdf.loc[:,[identifier,'cluster']].set_index(identifier)\n",
    "        if 'index' in tdf.columns:\n",
    "            return tdf.drop('index',axis=1)\n",
    "        return tdf\n",
    "\n",
    "    def bin_tweet_features(self,tweet_df,to_discretize,n_bins=10):\n",
    "        #stuff to skip because it's ordinal\n",
    "        x_discrete = tweet_df.loc[:,to_discretize].values\n",
    "        x_discrete = KBinsDiscretizer(n_bins,encode='ordinal',strategy='quantile').fit_transform(x_discrete)\n",
    "        tdf = tweet_df.copy()\n",
    "        tdf.loc[:,to_discretize] = x_discrete.astype('int')\n",
    "        return tdf\n",
    "    \n",
    "    def bin_demographics(self,tweet_df,to_discretize,n_bins=10):\n",
    "        tdf = tweet_df.copy()\n",
    "        demographics = [x for x in to_discretize if x in self.demographic_fields]\n",
    "        discretizer = KBinsDiscretizer(n_bins,encode='ordinal')\n",
    "        x_fit = self.demographic_df.loc[:,demographics].values\n",
    "        discretizer.fit(x_fit)\n",
    "        tdf.loc[:,demographics] = discretizer.transform(tdf.loc[:,demographics].values)\n",
    "        return tdf,demographics\n",
    "    \n",
    "    def decile_count(self,values,n_bins):\n",
    "        decile_counts = []\n",
    "        for decile in range(n_bins):\n",
    "            in_decile = (values.astype(int) == int(decile)).sum()\n",
    "            decile_counts.append(in_decile)\n",
    "        return decile_counts\n",
    "            \n",
    "    def tweet_cluster_df(self,cluster_fields,n_clusters,n_bins=5):\n",
    "        #should already filter geolocated here\n",
    "        tcluster_df = self.get_tweet_clusters(cluster_fields,n_clusters)\n",
    "        to_discretize = ['deaths_per_capita','cases_per_capita','retweet_count']\n",
    "        tcluster_df = self.bin_tweet_features(tcluster_df,to_discretize,n_bins)\n",
    "        \n",
    "        tcluster_df,binned_fields = self.bin_demographics(tcluster_df,cluster_fields,n_bins)\n",
    "        \n",
    "        cluster_dict = {}\n",
    "        for cluster, subdf in tcluster_df.groupby('cluster'):\n",
    "            cluster_size = subdf.shape[0]\n",
    "            entry = {'total_tweets': cluster_size}\n",
    "            \n",
    "            for field in binned_fields + ['cases_per_capita','deaths_per_capita']:\n",
    "                values = subdf.loc[:,field]\n",
    "                entry[field + '_quantiles'] = self.decile_count(values,n_bins)\n",
    "                \n",
    "            for_sah_rt = subdf[subdf.for_sah == 1].loc[:,'retweet_count']\n",
    "            against_sah_rt = subdf[subdf.for_sah < 1].loc[:,'retweet_count']\n",
    "            entry['for_sah_rt_quantiles'] = self.decile_count(for_sah_rt,n_bins)\n",
    "            entry['against_sah_rt_quantiles'] = self.decile_count(against_sah_rt,n_bins)\n",
    "            \n",
    "            entry['is_blue'] = subdf.is_blue.sum()\n",
    "            entry['positive_sentiment'] = (subdf.sentiment_score > .05).sum()\n",
    "            entry['negative_sentiment'] = (subdf.sentiment_score < .05).sum()\n",
    "            cluster_dict[cluster] = entry\n",
    "        return pd.DataFrame(cluster_dict).T\n",
    "    \n",
    "    def load_tweetcluster_dict(self,cluster_fields,n_clusters,n_bins=5):\n",
    "        cluster_df = self.tweet_cluster_df(cluster_fields,n_clusters,n_bins)\n",
    "        return cluster_df.to_dict(orient='records')\n",
    "        \n",
    "data = DataProcessor()\n",
    "# data.load_frameview_dict()\n",
    "# data.load_timeline_dict()\n",
    "# data.load_county_dict()\n",
    "data.load_tweetcluster_dict(['net_dem_president_votes','urm_pct','cvap'],4)\n",
    "# data.get_tweet_clusters(['is_blue','cvap'],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.augmented_tweet_df.is_blue.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "df = data.filter_geolocated(data.augmented_tweet_df).set_index('case_id')\n",
    "tc = TweetClusterer(['is_blue','cvap'])\n",
    "tc.add_clusters(df,data.demographic_df)\n",
    "tc.tweets_in_cluster(1003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.demographic_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
