{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Constants import Constants\n",
    "import Utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import json\n",
    "from sklearn.preprocessing import KBinsDiscretizer, quantile_transform\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cvap', 'net_dem_president_votes', 'urm_pct'] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'against_sah_rt_quantiles': [142, 24, 0, 0, 0],\n",
       "  'cases_per_capita_quantiles': [213, 219, 194, 177, 156],\n",
       "  'cvap_quantiles': [0, 13, 39, 56, 851],\n",
       "  'deaths_per_capita_quantiles': [236, 204, 187, 149, 183],\n",
       "  'for_sah_rt_quantiles': [618, 175, 0, 0, 0],\n",
       "  'is_blue': 587,\n",
       "  'negative_sentiment': 487,\n",
       "  'net_dem_president_votes_quantiles': [249, 50, 29, 21, 610],\n",
       "  'positive_sentiment': 472,\n",
       "  'total_tweets': 959,\n",
       "  'urm_pct_quantiles': [11, 47, 189, 368, 344]},\n",
       " {'against_sah_rt_quantiles': [26, 6, 0, 0, 0],\n",
       "  'cases_per_capita_quantiles': [41, 36, 53, 60, 71],\n",
       "  'cvap_quantiles': [0, 0, 0, 0, 261],\n",
       "  'deaths_per_capita_quantiles': [31, 39, 59, 64, 68],\n",
       "  'for_sah_rt_quantiles': [177, 52, 0, 0, 0],\n",
       "  'is_blue': 249,\n",
       "  'negative_sentiment': 130,\n",
       "  'net_dem_president_votes_quantiles': [12, 0, 0, 0, 249],\n",
       "  'positive_sentiment': 131,\n",
       "  'total_tweets': 261,\n",
       "  'urm_pct_quantiles': [0, 0, 8, 33, 220]},\n",
       " {'against_sah_rt_quantiles': [19, 2, 0, 0, 0],\n",
       "  'cases_per_capita_quantiles': [23, 33, 27, 37, 37],\n",
       "  'cvap_quantiles': [0, 0, 0, 0, 157],\n",
       "  'deaths_per_capita_quantiles': [19, 37, 33, 57, 11],\n",
       "  'for_sah_rt_quantiles': [105, 31, 0, 0, 0],\n",
       "  'is_blue': 119,\n",
       "  'negative_sentiment': 80,\n",
       "  'net_dem_president_votes_quantiles': [38, 0, 0, 0, 119],\n",
       "  'positive_sentiment': 77,\n",
       "  'total_tweets': 157,\n",
       "  'urm_pct_quantiles': [0, 0, 0, 0, 157]},\n",
       " {'against_sah_rt_quantiles': [14, 1, 0, 0, 0],\n",
       "  'cases_per_capita_quantiles': [20, 8, 23, 22, 33],\n",
       "  'cvap_quantiles': [0, 0, 0, 0, 106],\n",
       "  'deaths_per_capita_quantiles': [11, 16, 18, 26, 35],\n",
       "  'for_sah_rt_quantiles': [62, 29, 0, 0, 0],\n",
       "  'is_blue': 106,\n",
       "  'negative_sentiment': 54,\n",
       "  'net_dem_president_votes_quantiles': [0, 0, 0, 0, 106],\n",
       "  'positive_sentiment': 52,\n",
       "  'total_tweets': 106,\n",
       "  'urm_pct_quantiles': [0, 0, 0, 0, 106]}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_static_county_data(file=Constants.static_county_data_output_file):\n",
    "    try:\n",
    "        with open(file,'r') as f:\n",
    "            df = json.load(f)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         df = processed_county_demographics()\n",
    "#         df.loc[:,'features'] = df.geometry.apply(lambda x: mapping(x))\n",
    "#         return pd.DataFrame(df).drop(['geometry'],axis=1).to_dict(orient='index')\n",
    "    \n",
    "def load_census_df(file=Constants.static_county_data_output_file):\n",
    "    df = pd.DataFrame(load_static_county_data()).T\n",
    "    df.index.name = 'GEOID'\n",
    "    df = df.drop(['county_fips','features'],axis=1).reset_index()\n",
    "    df.GEOID = df.GEOID.astype(int)\n",
    "    return df\n",
    "\n",
    "def make_geojson(feature_entry):\n",
    "    gj = {\"type\": \"Feature\", \"properties\": {}, \"geometry\": feature_entry}\n",
    "    return gj\n",
    "\n",
    "def load_county_border_dict(border_json=Constants.static_county_data_output_file):\n",
    "    #should give {geoid: <geojson item> for each county}\n",
    "    try:\n",
    "        with open(border_json,'r') as f:\n",
    "            border_dict = json.load(f)\n",
    "        borders = pd.DataFrame(border_dict).T\n",
    "        borders.index.name = \"GEOID\"\n",
    "        return borders.features.apply(make_geojson).to_dict()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def load_aggregate_border_dict(border_json=Constants.aggregated_county_border_output_file):\n",
    "    #should give {geoid: <geojson item> for each group of countys}\n",
    "    try:\n",
    "        with open(border_json,'r') as f:\n",
    "            aggregate_borders = json.load(f)\n",
    "        return aggregate_borders\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def get_date_blocks(n_days = 4):\n",
    "    blocks = []\n",
    "    curr_day = 1\n",
    "    while curr_day <= 30:\n",
    "        max_day = curr_day + n_days\n",
    "        if max_day > 29:\n",
    "            max_day = 32\n",
    "        new_block = np.arange(curr_day, max_day)\n",
    "        blocks.append(set(new_block))\n",
    "        curr_day = max_day\n",
    "    return blocks\n",
    "\n",
    "def get_date_string(month_int, day_int):\n",
    "            return str(month_int) + '/' + str(day_int) \n",
    "    \n",
    "class DataProcessor():\n",
    "    \n",
    "    def __init__(self, quant_bins = 5):\n",
    "        self.quant_bins = quant_bins\n",
    "        self.sentiment_threshold = .25\n",
    "        self.demographic_df = load_census_df()\n",
    "        self.demographic_fields = self.get_demographic_fields(self.demographic_df)\n",
    "        \n",
    "        self.tweet_df = pd.read_csv(Constants.tweet_output_file)#,dtype={'GEOID':int})\n",
    "        covid_df = pd.read_csv(Constants.covid_cases_output_file)#,dtype={'GEOID':int})\n",
    "        self.covid_df = covid_df.drop('Unnamed: 0', axis=1)\n",
    "        \n",
    "        self.augmented_tweet_df = self.augment_tweets()\n",
    "        \n",
    "        #currently loads in as a dict {id:<geojson string>,...}\n",
    "        self.county_borders = load_county_border_dict()\n",
    "        self.aggregate_borders = load_aggregate_border_dict()\n",
    "        \n",
    "        self.frames = self.get_frames()\n",
    "        self.current_tweet_clusters = None\n",
    "        \n",
    "    def get_frames(self):\n",
    "        frames = ['Authority','Betrayal',\n",
    "                  'Care','Degradation',\n",
    "                  'Fairness','Freedom',\n",
    "                  'Harm','Injustice',\n",
    "                  'Loyalty','Oppression',\n",
    "                  'Purity','Subversion'\n",
    "                 ]\n",
    "        return frames\n",
    "    \n",
    "    def get_demographic_fields(self, demographic_df):\n",
    "        #extract columns that are census/county numerical data\n",
    "        d_fields = set(demographic_df.columns)\n",
    "        extra_fields = set(['GEOID','parent','state_fips','county_name'])\n",
    "        return list(d_fields - extra_fields)\n",
    "    \n",
    "    def discretize(self, series, quant_bins = None):\n",
    "        if quant_bins is None:\n",
    "            quant_bins = self.quant_bins\n",
    "        discretizer = KBinsDiscretizer(quant_bins,encode='ordinal',strategy='uniform')\n",
    "        discretizer.fit(series.dropna().values.reshape(-1,1))\n",
    "        return discretizer.transform(series.values.reshape(-1,1))\n",
    "    \n",
    "    def add_discretized_column(self,df,colname,quant_bins=None):\n",
    "        new_colname = colname + '_discrete'\n",
    "        ndf = df.copy()\n",
    "        ndf.loc[:,new_colname] = self.discretize(ndf.loc[:,colname],quant_bins)\n",
    "        return ndf\n",
    "    \n",
    "    def discretize_rt_count(self,rt_value):\n",
    "        #split rt count into 0, > 1 and >10 for plotting\n",
    "        if rt_value < 1:\n",
    "            return 0\n",
    "        elif rt_value < 10:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    def augment_tweets(self):\n",
    "        ddf = self.demographic_df.copy() #bin_edges = self.quantize_demographics(self.demographic_df, self.quant_bins) \n",
    "        tdf = self.tweet_df.copy()\n",
    "        cdf = self.covid_df.copy()\n",
    "        \n",
    "        ddf_to_drop = list(set(ddf.columns).intersection(set(['county_name','state_fips'])))\n",
    "        mdf = tdf.merge(ddf.drop(ddf_to_drop,axis=1),on='GEOID',how='left')\n",
    "        mdf = mdf.merge(cdf,on=['GEOID','day','month','year'],how='left')\n",
    "        \n",
    "        mdf = mdf[mdf.GEOID != 0]\n",
    "        mdf.loc[:,'is_blue'] = (mdf.net_dem_president_votes > 0).astype(int)\n",
    "        mdf.loc[:,'cases_per_capita'] = (mdf.cases/mdf.cvap)\n",
    "        to_discretize = ['cases_per_capita']\n",
    "        for col in to_discretize:\n",
    "            mdf = self.add_discretized_column(mdf, col, quant_bins = 10)\n",
    "            \n",
    "        mdf.loc[:,'rt_discrete'] = mdf.retweet_count.apply(self.discretize_rt_count)\n",
    "        return mdf\n",
    "    \n",
    "    def filter_geolocated(self, df):\n",
    "        return df[df.GEOID != 0].copy().fillna(-1)\n",
    "    \n",
    "    def compute_avg_sentiment(self, ddf):\n",
    "            weights = (ddf.rt_discrete + 1)**.5\n",
    "            sentiment = (ddf.sentiment_score * weights).sum()/weights.sum()\n",
    "            return sentiment\n",
    "    \n",
    "    def add_covid_to_counties(self, dem_df):\n",
    "        cdf = self.covid_df.sort_values(by=['year','month','day'],kind='mergesort')\n",
    "        ddf = dem_df.copy()\n",
    "        for geoid, subdf in ddf.groupby(\"GEOID\"):\n",
    "            sub_cdf = cdf[cdf.GEOID == geoid]\n",
    "            geo_idx = subdf.index\n",
    "            ddf.loc[geo_idx,'median_cases'] = sub_cdf.cases.median()\n",
    "            ddf.loc[geo_idx,'median_deaths'] = sub_cdf.deaths.median()\n",
    "            ddf.loc[geo_idx,'max_cases'] = sub_cdf.cases.max()\n",
    "            ddf.loc[geo_idx,'max_deaths'] = sub_cdf.deaths.max()\n",
    "            \n",
    "            first = sub_cdf.iloc[0]\n",
    "            last = sub_cdf.iloc[-1]\n",
    "            ddf.loc[geo_idx,'cases_change'] = last.cases - first.cases\n",
    "            ddf.loc[geo_idx,'deaths_change'] = last.deaths - first.deaths\n",
    "        return ddf\n",
    "    \n",
    "    def format_county_data(self):\n",
    "        #data for the county map\n",
    "        tdf = self.filter_geolocated(self.augmented_tweet_df.copy())\n",
    "        ddf = self.demographic_df.copy()\n",
    "        ddf = self.add_covid_to_counties(ddf)\n",
    "        \n",
    "        key = 'GEOID'\n",
    "        ddf = ddf.set_index(key)\n",
    "        \n",
    "        frames = self.get_frames()\n",
    "        \n",
    "        data_dict = {}\n",
    "        for geoid, dem_subdf in ddf.groupby(key):\n",
    "            tweet_subdf = tdf[tdf[key] == geoid]\n",
    "            if tweet_subdf.shape[0] == 0:\n",
    "                entry = {f: 0 for f in frames}\n",
    "                entry['avg_sentiment'] = 0\n",
    "            else:\n",
    "                entry = tweet_subdf.loc[:,frames].sum().to_dict()\n",
    "                entry['avg_sentiment'] = self.compute_avg_sentiment(tweet_subdf)\n",
    "                \n",
    "            entry['total_tweets'] = tweet_subdf.shape[0]\n",
    "        \n",
    "            demographics = ddf.loc[geoid]\n",
    "            copy_fields = self.demographic_fields + ['median_cases','max_cases','cases_change']\n",
    "            for dem in copy_fields:\n",
    "                entry[dem] = demographics[dem]\n",
    "            data_dict[geoid] = entry\n",
    "        data = pd.DataFrame(data_dict)\n",
    "        return data\n",
    "    \n",
    "    def load_county_dict(self):\n",
    "        #issue: I'm not aggregating stuff properly????????\n",
    "        county_df = self.format_county_data().T\n",
    "#         if aggregate:\n",
    "#             map_json = self.aggregate_borders\n",
    "#         else:\n",
    "        map_json = self.county_borders\n",
    "            \n",
    "        county_df.index.name = 'GEOID'\n",
    "        county_dict = county_df.reset_index().to_dict(orient='records')\n",
    "        return {'demographics': county_dict, 'borders': map_json}\n",
    "    \n",
    "    def stratify_retweet_thresholds(self, n_quantiles = 3, retweet_col='retweet_count'):\n",
    "        tdf = self.tweet_df.copy()\n",
    "        retweets = tdf[tdf[retweet_col] > 1]\n",
    "        retweets = retweets[retweet_col]\n",
    "        quantile_edges =np.quantile(retweets, np.linspace(0,1,n_quantiles), interpolation='nearest')\n",
    "        quantile_edges = sorted(set([0,1]).union(set(quantile_edges)))\n",
    "        quantile_edges = [int(q)  for q in quantile_edges]\n",
    "        return quantile_edges\n",
    "    \n",
    "    def format_frameview_df(self):\n",
    "        #file to get data formated for the moral-frame view.  need to add int sentiment later\n",
    "        #only geolocated tweets I guess\n",
    "        tweet_df = self.filter_geolocated(self.augmented_tweet_df)\n",
    "        #filter by month if needed.  I think I am not planning on doing that tho\n",
    "\n",
    "        rt_levels = sorted(np.unique(tweet_df.rt_discrete))\n",
    "        frame_data = {}\n",
    "\n",
    "        for frame in self.frames:\n",
    "            frame_df = tweet_df[tweet_df[frame] > 0]\n",
    "            entry = {'total_tweets': frame_df.shape[0]}\n",
    "            for_rt_quantiles = []\n",
    "            against_rt_quantiles = []\n",
    "            for rt_level in rt_levels:\n",
    "                subdf = frame_df[frame_df.rt_discrete == rt_level]\n",
    "                for_sah_q = (subdf.for_sah > 0).sum()\n",
    "                against_sah_q = (subdf.for_sah <= 0).sum()\n",
    "\n",
    "                for_rt_quantiles.append(for_sah_q)\n",
    "                against_rt_quantiles.append(against_sah_q)\n",
    "    \n",
    "            entry['vivid'] = frame_df.is_vivid.sum()\n",
    "            entry['for_sah'] = frame_df.for_sah.sum()\n",
    "            entry['is_blue'] = frame_df.is_blue.sum()\n",
    "            entry['for_sah_rt_quantiles'] = for_rt_quantiles\n",
    "            entry['against_sah_rt_quantiles'] = against_rt_quantiles\n",
    "            entry['positive_sentiment'] = (frame_df.sentiment_score > self.sentiment_threshold).sum()\n",
    "            entry['negative_sentiment'] = (frame_df.sentiment_score < -self.sentiment_threshold).sum()\n",
    "            #these are for future use maybe.  not efficient but it's like 5 values so i don't care\n",
    "            frame_data[frame] = entry\n",
    "        return pd.DataFrame(frame_data).T.fillna(-1)\n",
    "    \n",
    "    def load_frameview_dict(self):\n",
    "        #loads in the data for using in the view with overview dat afor each frame\n",
    "        frame_df = self.format_frameview_df()\n",
    "        return frame_df.to_dict(orient='index')\n",
    "    \n",
    "    def load_tweet_df(self, min_retweets = 0):\n",
    "        #get data to use for the big timeline and the clusters, at once\n",
    "        df = self.filter_geolocated(self.augmented_tweet_df)\n",
    "        \n",
    "        #drop extra features, but check that they're in the columns because there's no good way to do that in pandas?\n",
    "        to_drop = ['screen_name','user_id'] + self.demographic_fields\n",
    "        to_drop.remove('cvap')\n",
    "        df = df.drop(list(set(df.columns).intersection(set(to_drop))),axis=1)\n",
    "        print(to_drop, df.columns)\n",
    "        #filter stuff to the selected month and frame\n",
    "        #in case we want only popular tweets\n",
    "        df = df[df.retweet_count >= min_retweets]\n",
    "#         tweet_dict = {day:d.to_dict(orient='records') for day,d in df.groupby('day')}\n",
    "        return df\n",
    "    \n",
    "    def split_along_median(self, df, strat):\n",
    "    #look at the county stuff?  won't work good for cases tho\n",
    "        if 'cases' in strat or 'deaths' in strat:\n",
    "            strat_df = df.groupby('GEOID').max()\n",
    "        else:\n",
    "            strat_df = df.groupby('GEOID').first()\n",
    "        median = strat_df[strat].median()\n",
    "        upper_split = df[df[strat] > median]\n",
    "        lower_split = df[df[strat] <= median]\n",
    "        return [lower_split, upper_split]\n",
    "\n",
    "    def split_demographics(self, df, strat_list):\n",
    "        #stratifys along the median for each demographic variable.  can do it twice\n",
    "        #will add in group_num, in order of low-low, low-high, high-low, high-high, etc\n",
    "        splits = [df.copy()]\n",
    "        for strat in strat_list:\n",
    "            new_splits = []\n",
    "            for entry in splits:\n",
    "                new_split = self.split_along_median(entry, strat)\n",
    "                new_splits.extend(new_split)\n",
    "            splits = new_splits\n",
    "        for i, split in enumerate(splits):\n",
    "            nsplit = self.format_cluster_df(split, strat_list)\n",
    "            nsplit.loc[:,'group_num'] = i\n",
    "            splits[i] = nsplit\n",
    "        split_df = pd.concat(splits)\n",
    "        return split_df\n",
    "\n",
    "    def format_cluster_df(self, sdf, strat_list):\n",
    "        to_keep = ['cases_per_capita_discrete','retweet_count','rt_discrete','for_sah','is_vivid','is_blue']\n",
    "        to_keep = to_keep + self.frames + ['GEOID']\n",
    "        to_keep = list(set(to_keep + strat_list))\n",
    "        new_sdf = sdf.loc[:,to_keep]\n",
    "        return new_sdf\n",
    "\n",
    "    def load_cluster_dict(self, strat_list):\n",
    "        sdf = self.split_demographics(self.augmented_tweet_df, strat_list)\n",
    "        to_calibrate = strat_list + ['cases_per_capita_discrete','retweet_count']\n",
    "        maxes = {s: sdf.loc[:,s].max() for s in to_calibrate}\n",
    "        mins = {s: sdf.loc[:,s].min() for s in to_calibrate}\n",
    "        #data for showing each cluster\n",
    "        entrys = []\n",
    "        #map of the group of each geoid so I can send it up to use for brushing and linking\n",
    "        geoid_group_dict = {}\n",
    "        for group, subdf in sdf.groupby('group_num'):\n",
    "            entry = {}\n",
    "            entry['group_num'] = group\n",
    "            entry['total_tweets'] = sdf.shape[0]\n",
    "            for bool_val in ['for_sah','is_vivid','is_blue'] + self.frames:\n",
    "                entry[bool_val] = (sdf[bool_val] == 1).sum()\n",
    "            for geoid in np.unique(sdf.loc[:,'GEOID'].values):\n",
    "                geoid_group_dict[geoid] = group\n",
    "            for var in to_calibrate:\n",
    "                entry[var] = sorted(sdf.loc[:,var].tolist())\n",
    "            entrys.append(entry)\n",
    "        c_dict = {'data': entrys, 'geoid_group_dict': geoid_group_dict}\n",
    "        for var in to_calibrate:\n",
    "            c_dict[var] = {'max': maxes[var], 'min': mins[var]}\n",
    "        return c_dict\n",
    "    \n",
    "    def load_timeline_dict(self, n_days = 3):\n",
    "    #tweets for the timline\n",
    "    #data format: [{position, avg_sentiment, start_date, end_date, total_rt_for, total_rt_against, tweets_list}]\n",
    "    #tweets list format: [{tweet values},...]\n",
    "    #general format: {max_rt_discrete_for, max_rt_discrete_against, data}\n",
    "    #n_days is an approximate count of the days in a \"position\", rounds the values for the end of months so there is no month crossover\n",
    "    #total_rt_for/against are the discretize rt counts (0, 1-10, 10+)\n",
    "\n",
    "        frames = self.get_frames()\n",
    "\n",
    "        tdf = self.load_tweet_df()\n",
    "\n",
    "        sunique = lambda f: sorted(np.unique(tdf[f]))\n",
    "        months = sunique('month')\n",
    "        days = sunique('day')\n",
    "\n",
    "        date_blocks = get_date_blocks(n_days)\n",
    "        tweet_lists = []\n",
    "        curr_block = 0\n",
    "        max_rt_for = 0\n",
    "        max_rt_against = 0\n",
    "        for month in months:\n",
    "            mdf = tdf[tdf.month == month]\n",
    "            for db in date_blocks:\n",
    "                ddf = mdf[mdf.day.isin(db)]\n",
    "                if ddf.shape[0] == 0:\n",
    "                    continue\n",
    "        #         block_tweets = ddf.drop(['sentiment_score'],axis=1).to_dict(orient='records')\n",
    "                avg_sentiment = self.compute_avg_sentiment(ddf)\n",
    "                entry = {'pos': curr_block, 'avg_sentiment': avg_sentiment}\n",
    "                entry['start_date'] = get_date_string(month, min(db))\n",
    "                entry['end_date'] = get_date_string(month, max(db))\n",
    "                \n",
    "                entry['total_rt_for'] = ddf[ddf.for_sah == 1].rt_discrete.sum()\n",
    "                entry['total_rt_against'] = ddf[ddf.for_sah != 1].rt_discrete.sum()\n",
    "                max_rt_for = max(entry['total_rt_for'], max_rt_for)\n",
    "                max_rt_against = max(entry['total_rt_against'], max_rt_against)\n",
    "                \n",
    "                entry['tweets'] = ddf.drop(['sentiment_score'],axis=1).to_dict(orient='records')\n",
    "                curr_block += 1\n",
    "                tweet_lists.append(entry)\n",
    "        #add the max for/against for callibration purposes\n",
    "        return {'data': tweet_lists, 'max_rt_discrete_for': max_rt_for, 'max_rt_discrete_against': max_rt_against}\n",
    "\n",
    "\n",
    "        \n",
    "data = DataProcessor()\n",
    "# data.load_county_dict()\n",
    "# data.load_frameview_dict()\n",
    "# data.load_timeline_dict()\n",
    "data.load_cluster_dict(['net_dem_gov_votes','cases_per_capita_discrete'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.augmented_tweet_df.is_blue.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "df = data.filter_geolocated(data.augmented_tweet_df).set_index('case_id')\n",
    "tc = TweetClusterer(['is_blue','cvap'])\n",
    "tc.add_clusters(df,data.demographic_df)\n",
    "tc.tweets_in_cluster(1003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.demographic_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
